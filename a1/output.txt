nmt-master/nmt/attention_model.py
1 # Copyright 2017 Google Inc. All Rights Reserved.
2 #
4 # you may not use this file except in compliance with the License.
5 # You may obtain a copy of the License at
6 #
7 #     http://www.apache.org/licenses/LICENSE-2.0
8 #
9 # Unless required by applicable law or agreed to in writing, software
11 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12 # See the License for the specific language governing permissions and
13 # limitations under the License.
14 # ==============================================================================
15 """Attention-based sequence-to-sequence model with dynamic RNN support."""
29   """Sequence-to-sequence dynamic model with attention.

  This class implements a multi-layer recurrent neural network as encoder,
  and an attention-based decoder. This is the same as the model described in
  (Luong et al., EMNLP'2015) paper: https://arxiv.org/pdf/1508.04025v5.pdf.
  This class also allows to use GRU cells in addition to LSTM cells with
  support for dropout.
  """
49     # Set attention_mechanism_fn
79     """Build a RNN cell with attention mechanism that can be used by decoder."""
80     # No Attention
95     # Ensure memory is batch-major
110     # Attention
125     # Only generate alignment in greedy INFER mode.
136     # TODO(thangluong): do we need num_layers, num_gpus?
157   """Create attention mechanism based on the attention_option."""
158   del mode  # unused
160   # Mechanism
186   """create attention image and attention summary."""
188   # Reshape to (batch, src_seq_len, tgt_seq_len,1)
191   # Scale to range [0, 255]
nmt-master/nmt/gnmt_model.py
1 # Copyright 2017 Google Inc. All Rights Reserved.
2 #
4 # you may not use this file except in compliance with the License.
5 # You may obtain a copy of the License at
6 #
7 #     http://www.apache.org/licenses/LICENSE-2.0
8 #
9 # Unless required by applicable law or agreed to in writing, software
11 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12 # See the License for the specific language governing permissions and
13 # limitations under the License.
14 # ==============================================================================
16 """GNMT attention sequence-to-sequence model with dynamic RNN support."""
32   """Sequence-to-sequence dynamic model with GNMT attention architecture.
  """
58     """Build a GNMT encoder."""
65     # Build GNMT encoder.
83       # Execute _build_bidirectional_rnn from Model class
90           num_bi_residual_layers=0,  # no residual connection
93       # Build unidirectional layers
101       # Pass all encoder states to the decoder
102       #   except the first bi-directional layer
110     """Build encoder layers all at once."""
129     # Use the top layer for now
136     """Run each of the encoder layer separately, not used in general seq2seq."""
172     """Build a RNN cell with GNMT attention architecture."""
173     # Standard attention
178     # GNMT attention
203     cell_list = model_helper._cell_list(  # pylint: disable=protected-access
216     # Only wrap the bottom layer with the attention mechanism.
219     # Only generate alignment in greedy INFER mode.
225         attention_layer_size=None,  # don't use attention layer.
262   """A MultiCell with GNMT attention style."""
265     """Creates a GNMTAttentionMultiCell.

    Args:
      attention_cell: An instance of AttentionWrapper.
      cells: A list of RNNCell wrapped with AttentionInputWrapper.
      use_new_attention: Whether to use the attention generated from current
        step bottom layer's output. Default is False.
    """
278     """Run the cell with bottom layer's attention copied to all upper layers."""
311   """Residual function that handles different inputs and outputs inner dims.

  Args:
    inputs: cell inputs, this is actual inputs concatenated with the attention
      vector.
    outputs: cell outputs

  Returns:
    outputs + actual inputs
  """
nmt-master/nmt/inference.py
1 # Copyright 2017 Google Inc. All Rights Reserved.
2 #
4 # you may not use this file except in compliance with the License.
5 # You may obtain a copy of the License at
6 #
7 #     http://www.apache.org/licenses/LICENSE-2.0
8 #
9 # Unless required by applicable law or agreed to in writing, software
11 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12 # See the License for the specific language governing permissions and
13 # limitations under the License.
14 # ==============================================================================
16 """To perform inference on test set given a trained model."""
40   """Decoding only a specific set of sentences."""
50       # get text translation
58       if infer_summary is not None:  # Attention models
72   """Load inference data."""
84   """Get the right model class depending on configuration."""
99   """Start session and load model."""
115   """Perform translation."""
150   """Inference with a single worker."""
153   # Read data
163     # Decode
197   """Inference using multiple workers."""
204   # Read data
207   # Split data to multiple workers
220     # Decode
235     # Change file name to indicate the file writing is completed.
238     # Job 0 is responsible for the clean up.
241     # Now write all translations
nmt-master/nmt/inference_test.py
1 # Copyright 2017 Google Inc. All Rights Reserved.
2 #
4 # you may not use this file except in compliance with the License.
5 # You may obtain a copy of the License at
6 #
7 #     http://www.apache.org/licenses/LICENSE-2.0
8 #
9 # Unless required by applicable law or agreed to in writing, software
11 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12 # See the License for the specific language governing permissions and
13 # limitations under the License.
14 # ==============================================================================
16 """Tests for model inference."""
38     # Prepare
47     # Create check point
115     # There are 5 examples, make batch_size=3 makes job0 has 3 examples, job1
116     # has 2 examples, and job2 has 0 example. This helps testing some edge
117     # cases.
129     # Note: Need to start job 0 at the end; otherwise, it will block the testing
130     # thread.
161     # TODO(rzhao): Make infer indices support batch_size > 1.
nmt-master/nmt/model.py
1 # Copyright 2017 Google Inc. All Rights Reserved.
2 #
4 # you may not use this file except in compliance with the License.
5 # You may obtain a copy of the License at
6 #
7 #     http://www.apache.org/licenses/LICENSE-2.0
8 #
9 # Unless required by applicable law or agreed to in writing, software
11 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12 # See the License for the specific language governing permissions and
13 # limitations under the License.
14 # ==============================================================================
16 """Basic sequence-to-sequence model with dynamic RNN support."""
41   """To allow for flexibily in returing different outputs."""
47   """To allow for flexibily in returing different outputs."""
54   """To allow for flexibily in returing different outputs."""
59   """Sequence-to-sequence base class.
  """
71     """Create the model.

    Args:
      hparams: Hyperparameter configurations.
      mode: TRAIN | EVAL | INFER
      iterator: Dataset Iterator that feeds data.
      source_vocab_table: Lookup table mapping source words to ids.
      target_vocab_table: Lookup table mapping target words to ids.
      reverse_target_vocab_table: Lookup table mapping ids to target words. Only
        required in INFER mode. Defaults to None.
      scope: scope of the model.
      extra_args: model_helper.ExtraArgs, for passing customizable functions.

    """
85     # Set params
90     # Not used in general seq2seq models; when True, ignore decoder & training
94     # Train graph
99     # Saver
111     """Set various params for self and initialize."""
130     # extra_args: to make it flexible for adding external customizable code
135     # Set num units
138     # Set num layers
144     # Set num residual layers
152     # Batch size
155     # Global step
158     # Initializer
164     # Embeddings
172     """Set up training and inference."""
186       ## Count the number of predicted words for compute ppl.
192     # Gradients and SGD update operation for training the model.
193     # Arrange for the embedding vars to appear at the beginning.
196       # warm-up
198       # decay
201       # Optimizer
209       # Gradients
223       # Summary
228     # Print trainable variables
236     """Get learning rate warmup."""
242     # Apply inverse decay if global steps less than warmup steps.
243     # Inspired by https://arxiv.org/pdf/1706.03762.pdf (Section 5.3)
244     # When step < warmup_steps,
245     #   learing_rate *= warmup_factor ** (warmup_steps - step)
247       # 0.01^(1/warmup_steps): we start with a lr, 100 times smaller
261     """Return decay info based on decay_scheme."""
275     elif not hparams.decay_scheme:  # no decay
284     """Get learning rate decay."""
302     """Init embeddings."""
320     """Get train summary."""
328     """Execute train graph."""
341     """Execute eval graph."""
349     """Subclass must implement this method.

    Creates a sequence-to-sequence model with dynamic RNN decoder API.
    Args:
      hparams: Hyperparameter configurations.
      scope: VariableScope for the created subgraph; default "dynamic_seq2seq".

    Returns:
      A tuple of the form (logits, loss_tuple, final_context_state, sample_id),
      where:
        logits: float32 Tensor [batch_size x num_decoder_symbols].
        loss: loss = the total loss / batch_size.
        final_context_state: the final state of decoder RNN.
        sample_id: sampling indices.

    Raises:
      ValueError: if encoder_type differs from mono and bi, or
        attention_option is not (luong | scaled_luong |
        bahdanau | normed_bahdanau).
    """
371     # Projection
379       # Encoder
380       if hparams.language_model:  # no encoder for language modeling
387       # Skip decoder if extracting only encoder layers
391       ## Decoder
395       ## Loss
407     """Subclass must implement this.

    Build and run an RNN encoder.

    Args:
      hparams: Hyperparameters configurations.

    Returns:
      A tuple of encoder_outputs and encoder_state.
    """
421     """Build a multi-layer RNN cell that can be used by encoder."""
436     """Maximum decoding steps at inference time."""
441       # TODO(thangluong): add decoding_length_factor flag
449     """Build and run a RNN decoder with a final projection layer.

    Args:
      encoder_outputs: The outputs of encoder for every time step.
      encoder_state: The final state of the encoder.
      hparams: The Hyperparameters configurations.

    Returns:
      A tuple of final logits and final decoder state:
        logits: size [time, batch_size, vocab_size] when time_major=True.
    """
466     # maximum_iteration: The maximum decoding steps.
470     ## Decoder.
476       # Optional ops depends on which mode we are in and which loss function we
477       # are using.
481       ## Train or eval
483         # decoder_emp_inp: [max_time, batch_size, num_units]
490         # Helper
495         # Decoder
501         # Dynamic decoding
511           # Note: this is required when using sampled_softmax_loss.
514         # Note: there's a subtle difference here between train and inference.
515         # We could have set output_layer when create my_decoder
516         #   and shared more code between train and inference.
517         # We chose to apply the output_layer to all timesteps for speed:
518         #   10% improvements for small models & 20% for larger ones.
519         # If memory is a concern, we should apply output_layer per timestep.
523         # Colocate output layer with the last RNN cell if there is no extra GPU
524         # available. Otherwise, put last layer on a separate GPU.
529           logits = tf.no_op()  # unused when using sampled softmax loss.
531       ## Inference
558           # Helper
578               output_layer=self.output_layer  # applied per timestep
581         # Dynamic decoding
604     """Subclass must implement this.

    Args:
      hparams: Hyperparameters configurations.
      encoder_outputs: The outputs of encoder for every time step.
      encoder_state: The final state of the encoder.
      source_sequence_length: sequence length of encoder_outputs.

    Returns:
      A tuple of a multi-layer RNN cell used by decoder and the intial state of
      the decoder RNN.
    """
620     """Compute softmax loss or sampled softmax loss."""
652     """Compute optimization loss."""
683     """Decode a batch.

    Args:
      sess: tensorflow session to use.

    Returns:
      A tuple consiting of outputs, infer_summary.
        outputs: of size [batch_size, time]
    """
696     # make sure outputs is of shape [batch_size, time] or [beam_width,
697     # batch_size, time] when using beam search.
701       # beam search output in [batch_size, time, beam_width] shape.
706     """Stack encoder states and return tensor [batch, length, layer, size]."""
714     # transform from [length, batch, ...] -> [batch, length, ...]
722   """Sequence-to-sequence dynamic model.

  This class implements a multi-layer recurrent neural network as encoder,
  and a multi-layer recurrent neural network decoder.
  """
728     """Build an encoder from a sequence.

    Args:
      hparams: hyperparameters.
      sequence: tensor with input sequence data.
      sequence_length: tensor with length of the input sequence.

    Returns:
      encoder_outputs: RNN encoder outputs.
      encoder_state: RNN encoder state.

    Raises:
      ValueError: if encoder_type is neither "uni" nor "bi".
    """
754       # Encoder_outputs: [max_time, batch_size, num_units]
786           # alternatively concat forward and backward states
789             encoder_state.append(bi_encoder_state[0][layer_id])  # forward
790             encoder_state.append(bi_encoder_state[1][layer_id])  # backward
795     # Use the top layer for now
801     """Build encoder from source."""
811     """Create and call biddirectional RNN cells.

    Args:
      num_residual_layers: Number of residual layers from top to bottom. For
        example, if `num_bi_layers=4` and `num_residual_layers=2`, the last 2 RNN
        layers in each RNN cell will be wrapped with `ResidualWrapper`.
      base_gpu: The gpu device id to use for the first forward RNN layer. The
        i-th forward RNN layer will use `(base_gpu + i) % num_gpus` as its
        device id. The `base_gpu` for backward RNN cell is `(base_gpu +
        num_bi_layers)`.

    Returns:
      The concatenated bidirectional output and the bidirectional RNN cell"s
      state.
    """
826     # Construct forward and backward cells
849     """Build an RNN cell that can be used by decoder."""
850     # We only make use of encoder_outputs in attention-based models
873     # For beam search, we need to replicate encoder infos beam_width times
nmt-master/nmt/model_helper.py
1 # Copyright 2017 Google Inc. All Rights Reserved.
2 #
4 # you may not use this file except in compliance with the License.
5 # You may obtain a copy of the License at
6 #
7 #     http://www.apache.org/licenses/LICENSE-2.0
8 #
9 # Unless required by applicable law or agreed to in writing, software
11 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12 # See the License for the specific language governing permissions and
13 # limitations under the License.
14 # ==============================================================================
16 """Utility functions for building models."""
39 # If a vocab size is greater than this value, put the embedding on cpu instead
44   """Create an initializer. init_weight is only for uniform."""
60   """Return a device string for multi-GPU setup."""
82   """Create train graph, model, and iterator."""
115     # Note: One can set model_device_fn to
116     # `tf.train.replica_device_setter(ps_tasks)` for distributed training.
144   """Create train graph, model, src/tgt file holders, and iterator."""
197   """Create inference model."""
238   """Decide on which device to place an embed matrix given its vocab size."""
248   """Load pretrain embeding from embed_file, and return an embedding matrix.

  Args:
    embed_file: Path to a Glove formated embedding txt file.
    num_trainable_tokens: Make the first n tokens in the vocab file as trainable
      variables. Default is 3, which is "<unk>", "<s>" and "</s>".
  """
280   """Create a new or load an existing embedding matrix."""
304   """Create embedding matrix for both encoder and decoder.

  Args:
    share_vocab: A boolean. Whether to share embedding matrix for both
      encoder and decoder.
    src_vocab_size: An integer. The source vocab size.
    tgt_vocab_size: An integer. The target vocab size.
    src_embed_size: An integer. The embedding dimension for the encoder's
      embedding.
    tgt_embed_size: An integer. The embedding dimension for the decoder's
      embedding.
    dtype: dtype of the embedding matrix. Default to float32.
    num_enc_partitions: number of partitions used for the encoder's embedding
      vars.
    num_dec_partitions: number of partitions used for the decoder's embedding
      vars.
    scope: VariableScope for the created subgraph. Default to "embedding".

  Returns:
    embedding_encoder: Encoder's embedding matrix.
    embedding_decoder: Decoder's embedding matrix.

  Raises:
    ValueError: if use share_vocab but source and target have different vocab
      size.
  """
333     # Note: num_partitions > 1 is required for distributed training due to
334     # embedding_lookup tries to colocate single partition-ed embedding variable
335     # with lookup ops. This may cause embedding variables being placed on worker
336     # jobs.
342     # Note: num_partitions > 1 is required for distributed training due to
343     # embedding_lookup tries to colocate single partition-ed embedding variable
344     # with lookup ops. This may cause embedding variables being placed on worker
345     # jobs.
360     # Share embedding
393   """Create an instance of a single RNN cell."""
394   # dropout (= 1 - keep_prob) is set to 0 during eval and infer
397   # Cell Type
419   # Dropout (= 1 - keep_prob)
426   # Residual
432   # Device Wrapper
444   """Create a list of RNN cells."""
448   # Multi-GPU
471   """Create multi-layer RNN cell.

  Args:
    unit_type: string representing the unit type, i.e. "lstm".
    num_units: the depth of each unit.
    num_layers: number of cells.
    num_residual_layers: Number of residual layers from top to bottom. For
      example, if `num_layers=4` and `num_residual_layers=2`, the last 2 RNN
      cells in the returned list will be wrapped with `ResidualWrapper`.
    forget_bias: the initial forget bias of the RNNCell(s).
    dropout: floating point value between 0.0 and 1.0:
      the probability of dropout.  this is ignored if `mode != TRAIN`.
    mode: either tf.contrib.learn.TRAIN/EVAL/INFER
    num_gpus: The number of gpus to use when performing round-robin
      placement of layers.
    base_gpu: The gpu device id to use for the first RNN cell in the
      returned list. The i-th RNN cell will use `(base_gpu + i) % num_gpus`
      as its device id.
    single_cell_fn: allow for adding customized cell.
      When not specified, we default to model_helper._single_cell
  Returns:
    An `RNNCell` instance.
  """
505   if len(cell_list) == 1:  # Single layer.
507   else:  # Multi layers
512   """Clipping gradients of a model."""
523   """Print a list of variables in a checkpoint together with their shapes."""
532   """Load model from a checkpoint."""
550   """Average the last N checkpoints in the model_dir."""
556   # Checkpoints are ordered from oldest to newest.
592   # Build a graph with same variables in the checkpoints, and save the averaged
593   # variables into the avg_model_dir.
612       # Use the built saver to save the averaged checkpoint. Only keep 1
613       # checkpoint and the best checkpoint will be moved to avg_best_metric_dir.
622   """Create translation model and initialize or load parameters in session."""
638   """Compute perplexity of the output of the model.

  Args:
    model: model for compute perplexity.
    sess: tensorflow session to use.
    name: name of the batch.

  Returns:
    The perplexity of the eval outputs.
  """
nmt-master/nmt/model_test.py
1 # Copyright 2017 Google Inc. All Rights Reserved.
2 #
4 # you may not use this file except in compliance with the License.
5 # You may obtain a copy of the License at
6 #
7 #     http://www.apache.org/licenses/LICENSE-2.0
8 #
9 # Unless required by applicable law or agreed to in writing, software
11 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12 # See the License for the specific language governing permissions and
13 # limitations under the License.
14 # ==============================================================================
15 """Tests for model.py."""
360   ## Testing 3 encoders:
361   # uni: no attention, no residual, 1 layers
362   # bi: no attention, with residual, 4 layers
374     # pylint: disable=line-too-long
384     # pylint: enable=line-too-long
431     # pylint: disable=line-too-long
453     # pylint: enable=line-too-long
489   ## Test attention mechanisms: luong, scaled_luong, bahdanau, normed_bahdanau
501     # pylint: disable=line-too-long
517     # pylint: enable=line-too-long
529           # pylint: disable=line-too-long
536           # pylint: enable=line-too-long
569     # pylint: disable=line-too-long
586     # pylint: enable=line-too-long
598           # pylint: disable=line-too-long
605           # pylint: enable=line-too-long
643     # pylint: disable=line-too-long
661     # pylint: enable=line-too-long
673           # pylint: disable=line-too-long
680           # pylint: enable=line-too-long
713     # pylint: disable=line-too-long
733     # pylint: enable=line-too-long
746           # pylint: disable=line-too-long
753           # pylint: enable=line-too-long
780   ## Test encoder vs. attention (all use residual):
781   # uni encoder, standard attention
792     # pylint: disable=line-too-long
817     # pylint: enable=line-too-long
860   # Test gnmt model.
871     # pylint: disable=line-too-long
897     # pylint: enable=line-too-long
940   # Test beam search.
nmt-master/nmt/nmt.py
1 # Copyright 2017 Google Inc. All Rights Reserved.
2 #
4 # you may not use this file except in compliance with the License.
5 # You may obtain a copy of the License at
6 #
7 #     http://www.apache.org/licenses/LICENSE-2.0
8 #
9 # Unless required by applicable law or agreed to in writing, software
11 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12 # See the License for the specific language governing permissions and
13 # limitations under the License.
14 # ==============================================================================
16 """TensorFlow NMT model implementation."""
24 # import matplotlib.image as mpimg
46   """Build ArgumentParser."""
49   # network
57   parser.add_argument("--encoder_type", type=str, default="uni", help="""\
      uni | bi | gnmt.
      For bi, we build num_encoder_layers/2 bi-directional layers.
      For gnmt, we build 1 bi-directional layer, and (num_encoder_layers - 1)
        uni-directional layers.\
      """)
72   # attention mechanisms
73   parser.add_argument("--attention", type=str, default="", help="""\
      luong | scaled_luong | bahdanau | normed_bahdanau or set to "" for no
      attention\
      """)
81       help="""\
      standard | gnmt | gnmt_v2.
      standard: use top layer to compute attention.
      gnmt: GNMT style of computing attention, use previous bottom layer to
          compute attention.
      gnmt_v2: similar to gnmt, but use current bottom layer to compute
          attention.\
      """)
92       help="""\
      Only used in standard attention_architecture. Whether use attention as
      the cell output at each timestep.
      .\
      """)
100       help="""\
      Whether to pass encoder's hidden state to decoder when using an attention
      based model.\
      """)
105   # optimizer
111   parser.add_argument("--warmup_scheme", type=str, default="t2t", help="""\
      How to warmup learning rates. Options include:
        t2t: Tensor2Tensor's way, start with lr 100 times smaller, then
             exponentiate until the specified lr.\
      """)
117       "--decay_scheme", type=str, default="", help="""\
      How we decay learning rate. Options include:
        luong234: after 2/3 num train steps, we start halving the learning rate
          for 4 times before finishing.
        luong5: after 1/2 num train steps, we start halving the learning rate
          for 5 times before finishing.\
        luong10: after 1/2 num train steps, we start halving the learning rate
          for 10 times before finishing.\
      """)
135   # initializer
142   # data
156   # Vocab
157   parser.add_argument("--vocab_prefix", type=str, default=None, help="""\
      Vocab prefix, expect files with src/tgt suffixes.\
      """)
160   parser.add_argument("--embed_prefix", type=str, default=None, help="""\
      Pretrained embedding prefix, expect files with src/tgt suffixes.
      The embedding files should be Glove formated txt files.\
      """)
170                       help="""\
      Whether to use the source vocab and embeddings for both source and
      target.\
      """)
175                       help="""\
                      Whether check special sos, eos, unk tokens exist in the
                      vocab files.\
                      """)
180   # Sequence lengths
188                       help="""\
      Max length of tgt sequences during inference.  Also use to restrict the
      maximum decoding length.\
      """)
193   # Default settings works well (rarely need to change)
215   # SPM
218                       help="""\
                      Set to bpe or spm to activate subword desegmentation.\
                      """)
222   # Experimental encoding feature.
224                       help="""\
                      Whether to split each word or bpe into character, and then
                      generate the word-level representation from the character
                      reprentation.
                      """)
230   # Misc
239                       help="""\
      How many training steps to do per external evaluation.  Automatically set
      based on data if None.\
      """)
256                       const=True, default=False, help=("""\
                      Average the last N checkpoints for external evaluation.
                      N can be controlled by setting --num_keep_ckpts.\
                      """))
264   # Inference
277                       help=("""\
      Reference file to compute evaluation scores (if provided).\
      """))
281   # Advanced inference arguments
286                       help=("""\
      beam width when using beam search decoder. If 0 (default), use standard
      decoder with greedy helper.\
      """))
296                       help=("""\
      Softmax sampling temperature for inference decoding, 0.0 means greedy
      decoding. This option is ignored when using beam search.\
      """))
301                       help=("""\
      Number of translations generated for each sentence. This is only used for
      inference.\
      """))
306   # Job info
318   """Create training hparams."""
320       # Data
330       # Networks
341       # Attention mechanisms
347       # Train
361       # Data constraints
367       # Inference
372       # Advanced inference arguments
380       # Vocab
387       # Misc
390       epoch_step=0,  # record where we were within an epoch.
407   """Add an argument to hparams; if exists, change the value if update==True."""
416   """Add new arguments to hparams."""
417   # Sanity checks
435   # Different number of encoder / decoder layers
444   # Set residual layers
454       # The first unidirectional layer (after the bi-directional layer) in
455       # the GNMT encoder can't have residual connection due to the input is
456       # the concatenation of fw_cell and bw_cell's outputs.
459       # Compatible for GNMT models
467   # Language modeling
477   ## Vocab
478   # Get vocab file names first
485   # Source vocab
495   # Target vocab
513   # Num embedding partitions
518   # Pretrained Embeddings
545   # Evaluation
562   """Make sure the loaded hparams is compatible with new changes."""
566   # Set num encoder/decoder layers (for old checkpoints)
573   # For compatible reason, if there are new fields in default_hparams,
574   #   we add them to the current hparams
581   # Update all hparams' keys if override_loaded_hparams=True
585     # For inference
599   """Create hparams or load hparams from out_dir."""
609   # Save HParams
615   # Print HParams
621   """Run main."""
622   # Job
627   # GPU device
631   # Random
638   # Model output directory
644   # Load hparams.
646   if flags.ckpt:  # Try to load hparams from the same directory as ckpt
654   if not loaded_hparams:  # Try to load from out_dir
660   ## Train / Decode
662     # Inference output directory
668     # Inference indices
674     # Inference
681     # Evaluation
692     # Train
nmt-master/nmt/nmt_test.py
1 # Copyright 2017 Google Inc. All Rights Reserved.
2 #
4 # you may not use this file except in compliance with the License.
5 # You may obtain a copy of the License at
6 #
7 #     http://www.apache.org/licenses/LICENSE-2.0
8 #
9 # Unless required by applicable law or agreed to in writing, software
11 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12 # See the License for the specific language governing permissions and
13 # limitations under the License.
14 # ==============================================================================
15 """Tests for nmt.py, train.py and inference.py."""
32   """Update flags for basic training."""
51     """Test the training loop is functional with basic hparams."""
65     """Test the training loop is functional with basic hparams."""
80     """Test inference is function with basic hparams."""
87     # Train one step so we have a checkpoint.
93     # Update FLAGS for inference.
nmt-master/nmt/scripts/bleu.py
1 # Copyright 2017 Google Inc. All Rights Reserved.
2 #
4 # you may not use this file except in compliance with the License.
5 # You may obtain a copy of the License at
6 #
7 #     http://www.apache.org/licenses/LICENSE-2.0
8 #
9 # Unless required by applicable law or agreed to in writing, software
11 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12 # See the License for the specific language governing permissions and
13 # limitations under the License.
14 # ==============================================================================
16 """Python implementation of BLEU and smooth-BLEU.

This module provides a Python implementation of BLEU and smooth-BLEU.
Smooth BLEU is computed following the method outlined in the paper:
Chin-Yew Lin, Franz Josef Och. ORANGE: a method for evaluating automatic
evaluation metrics for machine translation. COLING 2004.
"""
29   """Extracts all n-grams upto a given maximum order from an input segment.

  Args:
    segment: text segment from which n-grams will be extracted.
    max_order: maximum length in tokens of the n-grams returned by this
        methods.

  Returns:
    The Counter containing all n-grams upto max_order in segment
    with a count of how many times each n-gram occurred.
  """
50   """Computes BLEU score of translated segments against one or more references.

  Args:
    reference_corpus: list of lists of references for each translation. Each
        reference should be tokenized into a list of tokens.
    translation_corpus: list of translations to score. Each translation
        should be tokenized into a list of tokens.
    max_order: Maximum n-gram order to use when computing BLEU score.
    smooth: Whether or not to apply Lin et al. 2004 smoothing.

  Returns:
    3-Tuple with the BLEU score, n-gram precisions, geometric mean of n-gram
    precisions and brevity penalty.
  """
nmt-master/nmt/scripts/rouge.py
1 """ROUGE metric implementation.

Copy from tf_seq2seq/seq2seq/metrics/rouge.py.
This is a modified and slightly extended verison of
https://github.com/miso-belica/sumy/blob/dev/sumy/evaluation/rouge.py.
"""
16 #pylint: disable=C0103
20   """Calcualtes n-grams.

  Args:
    n: which n-grams to calculate
    text: An array of tokens

  Returns:
    A set of n-grams
  """
38   """Splits multiple sentences into words and flattens the result"""
43   """Calculates word n-grams for multiple sentences.
  """
53   """
  Returns the length of the Longest Common Subsequence between sequences x
  and y.
  Source: http://www.algorithmist.com/index.php/Longest_Common_Subsequence

  Args:
    x: sequence of words
    y: sequence of words

  Returns
    integer: Length of LCS between x and y
  """
71   """
  Computes the length of the longest common subsequence (lcs) between two
  strings. The implementation below uses a DP programming algorithm and runs
  in O(nm) time where n = len(x) and m = len(y).
  Source: http://www.algorithmist.com/index.php/Longest_Common_Subsequence

  Args:
    x: collection of words
    y: collection of words

  Returns:
    Table of dictionary of coord and len lcs
  """
98   """
  Returns the Longest Subsequence between x and y.
  Source: http://www.algorithmist.com/index.php/Longest_Common_Subsequence

  Args:
    x: sequence of words
    y: sequence of words

  Returns:
    sequence: LCS of x and y
  """
113     """private recon calculation"""
128   """
  Computes ROUGE-N of two text collections of sentences.
  Sourece: http://research.microsoft.com/en-us/um/people/cyl/download/
  papers/rouge-working-note-v1.3.1.pdf

  Args:
    evaluated_sentences: The sentences that have been picked by the summarizer
    reference_sentences: The sentences from the referene set
    n: Size of ngram.  Defaults to 2.

  Returns:
    A tuple (f1, precision, recall) for ROUGE-N

  Raises:
    ValueError: raises exception if a param has len <= 0
  """
152   # Gets the overlapping ngrams between evaluated and reference
156   # Handle edge case. This isn't mathematically correct, but it's good enough
169   # return overlapping_count / reference_count
174   """
  Computes the LCS-based F-measure score
  Source: http://research.microsoft.com/en-us/um/people/cyl/download/papers/
  rouge-working-note-v1.3.1.pdf

  Args:
    llcs: Length of LCS
    m: number of words in reference summary
    n: number of words in candidate summary

  Returns:
    Float. LCS-based F-measure score
  """
197   """
  Computes ROUGE-L (sentence level) of two text collections of sentences.
  http://research.microsoft.com/en-us/um/people/cyl/download/papers/
  rouge-working-note-v1.3.1.pdf

  Calculated according to:
  R_lcs = LCS(X,Y)/m
  P_lcs = LCS(X,Y)/n
  F_lcs = ((1 + beta^2)*R_lcs*P_lcs) / (R_lcs + (beta^2) * P_lcs)

  where:
  X = reference summary
  Y = Candidate summary
  m = length of reference summary
  n = length of candidate summary

  Args:
    evaluated_sentences: The sentences that have been picked by the summarizer
    reference_sentences: The sentences from the referene set

  Returns:
    A float: F_lcs

  Raises:
    ValueError: raises exception if a param has len <= 0
  """
234   """
  Returns LCS_u(r_i, C) which is the LCS score of the union longest common
  subsequence between reference sentence ri and candidate summary C. For example
  if r_i= w1 w2 w3 w4 w5, and C contains two sentences: c1 = w1 w2 w6 w7 w8 and
  c2 = w1 w3 w8 w9 w5, then the longest common subsequence of r_i and c1 is
  "w1 w2" and the longest common subsequence of r_i and c2 is "w1 w3 w5". The
  union longest common subsequence of r_i, c1, and c2 is "w1 w2 w3 w5" and
  LCS_u(r_i, C) = 4/5.

  Args:
    evaluated_sentences: The sentences that have been picked by the summarizer
    reference_sentence: One of the sentences in the reference summaries

  Returns:
    float: LCS_u(r_i, C)

  ValueError:
    Raises exception if a param has len <= 0
  """
271   """
  Computes ROUGE-L (summary level) of two text collections of sentences.
  http://research.microsoft.com/en-us/um/people/cyl/download/papers/
  rouge-working-note-v1.3.1.pdf

  Calculated according to:
  R_lcs = SUM(1, u)[LCS<union>(r_i,C)]/m
  P_lcs = SUM(1, u)[LCS<union>(r_i,C)]/n
  F_lcs = ((1 + beta^2)*R_lcs*P_lcs) / (R_lcs + (beta^2) * P_lcs)

  where:
  SUM(i,u) = SUM from i through u
  u = number of sentences in reference summary
  C = Candidate summary made up of v sentences
  m = number of words in reference summary
  n = number of words in candidate summary

  Args:
    evaluated_sentences: The sentences that have been picked by the summarizer
    reference_sentence: One of the sentences in the reference summaries

  Returns:
    A float: F_lcs

  Raises:
    ValueError: raises exception if a param has len <= 0
  """
301   # total number of words in reference sentences
304   # total number of words in evaluated sentences
315   """Calculates average rouge scores for a list of hypotheses and
  references"""
318   # Filter out hyps that are of 0 length
319   # hyps_and_refs = zip(hypotheses, references)
320   # hyps_and_refs = [_ for _ in hyps_and_refs if len(_[0]) > 0]
321   # hypotheses, references = zip(*hyps_and_refs)
323   # Calculate ROUGE-1 F1, precision, recall scores
329   # Calculate ROUGE-2 F1, precision, recall scores
335   # Calculate ROUGE-L F1, precision, recall scores
nmt-master/nmt/scripts/__init__.py
nmt-master/nmt/train.py
1 # Copyright 2017 Google Inc. All Rights Reserved.
2 #
4 # you may not use this file except in compliance with the License.
5 # You may obtain a copy of the License at
6 #
7 #     http://www.apache.org/licenses/LICENSE-2.0
8 #
9 # Unless required by applicable law or agreed to in writing, software
11 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12 # See the License for the specific language governing permissions and
13 # limitations under the License.
14 # ==============================================================================
15 """For training NMT models."""
45   """Sample decode a random sentence from src_data."""
64   """Compute internal evaluation (perplexity) for both dev / test.

  Computes development and testing perplexities for given model.

  Args:
    eval_model: Evaluation model for which to compute perplexities.
    eval_sess: Evaluation TensorFlow session.
    model_dir: Directory from which to load evaluation model from.
    hparams: Model hyper-parameters.
    summary_writer: Summary writer for logging metrics to TensorBoard.
    use_test_set: Computes testing perplexity if true; does not otherwise.
      Note that the development perplexity is always computed regardless of
      value of this parameter.
    dev_eval_iterator_feed_dict: Feed dictionary for a TensorFlow session.
      Can be used to pass in additional inputs necessary for running the
      development evaluation.
    test_eval_iterator_feed_dict: Feed dictionary for a TensorFlow session.
      Can be used to pass in additional inputs necessary for running the
      testing evaluation.
  Returns:
    Pair containing development perplexity and testing perplexity, in this
    order.
  """
127   """Compute external evaluation for both dev / test.

  Computes development and testing external evaluation (e.g. bleu, rouge) for
  given model.

  Args:
    infer_model: Inference model for which to compute perplexities.
    infer_sess: Inference TensorFlow session.
    model_dir: Directory from which to load inference model from.
    hparams: Model hyper-parameters.
    summary_writer: Summary writer for logging metrics to TensorBoard.
    use_test_set: Computes testing external evaluation if true; does not
      otherwise. Note that the development external evaluation is always
      computed regardless of value of this parameter.
    dev_infer_iterator_feed_dict: Feed dictionary for a TensorFlow session.
      Can be used to pass in additional inputs necessary for running the
      development external evaluation.
    test_infer_iterator_feed_dict: Feed dictionary for a TensorFlow session.
      Can be used to pass in additional inputs necessary for running the
      testing external evaluation.
  Returns:
    Triple containing development scores, testing scores and the TensorFlow
    Variable for the global step number, in this order.
  """
203   """Creates an averaged checkpoint and run external eval with it."""
206     # Convert VariableName:0 to VariableName.
235   """Compute internal evaluation (perplexity) for both dev / test.

  Computes development and testing perplexities for given model.

  Args:
    model_dir: Directory from which to load models from.
    infer_model: Inference model for which to compute perplexities.
    infer_sess: Inference TensorFlow session.
    eval_model: Evaluation model for which to compute perplexities.
    eval_sess: Evaluation TensorFlow session.
    hparams: Model hyper-parameters.
    summary_writer: Summary writer for logging metrics to TensorBoard.
    avg_ckpts: Whether to compute average external evaluation scores.
    dev_eval_iterator_feed_dict: Feed dictionary for a TensorFlow session.
      Can be used to pass in additional inputs necessary for running the
      internal development evaluation.
    test_eval_iterator_feed_dict: Feed dictionary for a TensorFlow session.
      Can be used to pass in additional inputs necessary for running the
      internal testing evaluation.
    dev_infer_iterator_feed_dict: Feed dictionary for a TensorFlow session.
      Can be used to pass in additional inputs necessary for running the
      external development evaluation.
    test_infer_iterator_feed_dict: Feed dictionary for a TensorFlow session.
      Can be used to pass in additional inputs necessary for running the
      external testing evaluation.
  Returns:
    Triple containing results summary, global step Tensorflow Variable and
    metrics in this order.
  """
320   """Wrapper for running sample_decode, internal_eval and external_eval.

  Args:
    model_dir: Directory from which to load models from.
    infer_model: Inference model for which to compute perplexities.
    infer_sess: Inference TensorFlow session.
    eval_model: Evaluation model for which to compute perplexities.
    eval_sess: Evaluation TensorFlow session.
    hparams: Model hyper-parameters.
    summary_writer: Summary writer for logging metrics to TensorBoard.
    sample_src_data: sample of source data for sample decoding.
    sample_tgt_data: sample of target data for sample decoding.
    avg_ckpts: Whether to compute average external evaluation scores.
  Returns:
    Triple containing results summary, global step Tensorflow Variable and
    metrics in this order.
  """
345   """Initialize statistics that we want to accumulate."""
354   """Update stats: write summary and accumulate statistics."""
357   # Update statistics
371   """Print all info at the current global step."""
381   """Add stuffs in info to summaries."""
389   """Update info and check for overflow."""
390   # Per-step info
396   # Per-predict info
400   # Check for overflow
413   """Misc tasks to do before training."""
425   # Initialize all of the iterators
436   """Get the right model class depending on configuration."""
451   """Train a translation model."""
463   # Create model
469   # Preload data for sample decoding.
478   # Log and output files
483   # TensorFlow model
499   # Summary writer
503   # First evaluation
514   # This is the training loop.
518     ### Run a step ###
524       # Finished going through the training dataset.  Go to next epoch.
543     # Process step_result, accumulate stats, and write summary
548     # Once in a while, we print statistics.
558       # Reset statistics
566       # Save checkpoint
572       # Evaluate on dev/test
582       # Save checkpoint
598   # Done training
640   """Format results."""
654   """Summary of the current best results."""
663   """Computing perplexity."""
673   """Pick a sentence and decode."""
686     # get the top translation.
698   # Summary
706   """External evaluation such as BLEU and ROUGE scores."""
731   # Save on best metrics
741       # metric: larger is better
nmt-master/nmt/utils/common_test_utils.py
1 # Copyright 2017 Google Inc. All Rights Reserved.
2 #
4 # you may not use this file except in compliance with the License.
5 # You may obtain a copy of the License at
6 #
7 #     http://www.apache.org/licenses/LICENSE-2.0
8 #
9 # Unless required by applicable law or agreed to in writing, software
11 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12 # See the License for the specific language governing permissions and
13 # limitations under the License.
14 # ==============================================================================
16 """Common utility functions for tests."""
40   """Create training and inference test hparams."""
43     # TODO(rzhao): Put num_residual_layers computation logic into
44     # `model_utils.py`, so we can also test it here.
49   # Networks
59   # Attention mechanisms
63   # Train
68   # Infer
73   # Misc
78   # Vocab
88   # For inference.py test
99   """Create test iterator."""
nmt-master/nmt/utils/evaluation_utils.py
1 # Copyright 2017 Google Inc. All Rights Reserved.
2 #
4 # you may not use this file except in compliance with the License.
5 # You may obtain a copy of the License at
6 #
7 #     http://www.apache.org/licenses/LICENSE-2.0
8 #
9 # Unless required by applicable law or agreed to in writing, software
11 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12 # See the License for the specific language governing permissions and
13 # limitations under the License.
14 # ==============================================================================
16 """Utility for evaluating various tasks, e.g., translation & summarization."""
32   """Pick a metric and evaluate depending on task."""
33   # BLEU scores for translation task
37   # ROUGE scores for summarization tasks
52   """Clean and handle BPE or SPM outputs."""
55   # BPE
59   # SPM
66 # Follow //transconsole/localization/machine_translation/metrics/bleu_calc.py
68   """Compute BLEU scores and handling BPE."""
93   # bleu_score, precisions, bp, ratio, translation_length, reference_length
100   """Compute ROUGE scores and handling BPE."""
118   """Compute accuracy, each line contains a label."""
134   """Compute accuracy on per word basis."""
154   """Compute BLEU scores using Moses multi-bleu.perl script."""
156   # TODO(thangluong): perform rewrite using python
157   # BPE
161       # TODO(thangluong): not use shell=True, can be a security hazard
176   # subprocess
177   # TODO(thangluong): not use shell=True, can be a security hazard
180   # extract BLEU score
nmt-master/nmt/utils/evaluation_utils_test.py
1 # Copyright 2017 Google Inc. All Rights Reserved.
2 #
4 # you may not use this file except in compliance with the License.
5 # You may obtain a copy of the License at
6 #
7 #     http://www.apache.org/licenses/LICENSE-2.0
8 #
9 # Unless required by applicable law or agreed to in writing, software
11 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12 # See the License for the specific language governing permissions and
13 # limitations under the License.
14 # ==============================================================================
16 """Tests for evaluation_utils.py."""
nmt-master/nmt/utils/iterator_utils.py
1 # Copyright 2017 Google Inc. All Rights Reserved.
2 #
4 # you may not use this file except in compliance with the License.
5 # You may obtain a copy of the License at
6 #
7 #     http://www.apache.org/licenses/LICENSE-2.0
8 #
9 # Unless required by applicable law or agreed to in writing, software
11 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12 # See the License for the specific language governing permissions and
13 # limitations under the License.
14 # ==============================================================================
15 """For loading data into NMT models."""
28 # NOTE(ebrevdo): When we subclass this, instances' __dict__ becomes empty.
53     # Convert the word strings to character ids
57     # Convert the word strings to ids
61   # Add in the word counts.
73         # The entry is the source line rows;
74         # this has unknown-length vectors.  The last entry is
75         # the source row size; this is a scalar.
77             tf.TensorShape([None]),  # src
78             tf.TensorShape([])),  # src_len
79         # Pad the source sequences with eos tokens.
80         # (Though notice we don't generally need to do this since
81         # later on we will be masking out calculations past the true sequence.
83             src_eos_id,  # src
84             0))  # src_len -- unused
141   # Filter zero length input sequences.
154   # Convert the word strings to ids.  Word strings that are not in the
155   # vocab get the lookup table's default_value integer.
168   # Create a tgt_input prefixed with <sos> and a tgt_output suffixed with <eos>.
174   # Add in sequence lengths.
190   # Bucket by source sequence length (buckets for lengths 0-9, 10-19, ...)
194         # The first three entries are the source and target line rows;
195         # these have unknown-length vectors.  The last two entries are
196         # the source and target row sizes; these are scalars.
198             tf.TensorShape([None]),  # src
199             tf.TensorShape([None]),  # tgt_input
200             tf.TensorShape([None]),  # tgt_output
201             tf.TensorShape([]),  # src_len
202             tf.TensorShape([])),  # tgt_len
203         # Pad the source and target sequences with eos tokens.
204         # (Though notice we don't generally need to do this since
205         # later on we will be masking out calculations past the true sequence.
207             src_eos_id,  # src
208             tgt_eos_id,  # tgt_input
209             tgt_eos_id,  # tgt_output
210             0,  # src_len -- unused
211             0))  # tgt_len -- unused
216       # Calculate bucket_width by maximum source sequence length.
217       # Pairs with length [0, bucket_width) go to bucket 0, length
218       # [bucket_width, 2 * bucket_width) go to bucket 1, etc.  Pairs with length
219       # over ((num_bucket-1) * bucket_width) words all go into the last bucket.
225       # Bucket sentence pairs by the length of their source sentence and target
226       # sentence.
nmt-master/nmt/utils/iterator_utils_test.py
1 # Copyright 2017 Google Inc. All Rights Reserved.
2 #
4 # you may not use this file except in compliance with the License.
5 # You may obtain a copy of the License at
6 #
7 #     http://www.apache.org/licenses/LICENSE-2.0
8 #
9 # Unless required by applicable law or agreed to in writing, software
11 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12 # See the License for the specific language governing permissions and
13 # limitations under the License.
14 # ==============================================================================
16 """Tests for iterator_utils.py"""
77           [[2, 0, 3],   # c a eos -- eos is padding
82           [[4, 1, 2],   # sos b c
83            [4, 2, 2]],  # sos c c
86           [[1, 2, 3],   # b c eos
87            [2, 2, 3]],  # c c eos
95           [[2, 2, 0]],  # c c a
99           [[4, 0, 1]],  # sos a b
102           [[0, 1, 3]],  # a b eos
157           [[2, 0, 3],     # c a eos -- eos is padding
162           [[4, 1, 2],   # sos b c
163            [4, 2, 2]],  # sos c c
166           [[1, 2, 3],   # b c eos
167            [2, 2, 3]],  # c c eos
226           [[4, 2, 2]],   # sos c c
229           [[2, 2, 3]],   # c c eos
236       # Re-init iterator with skip_count=0.
244            [2, 0, 3]],   # c a eos -- eos is padding
248           [[4, 2, 2],   # sos c c
249            [4, 1, 2]],  # sos b c
252           [[2, 2, 3],   # c c eos
253            [1, 2, 3]],  # b c eos
261           [[2, 2, 0]],  # c c a
265           [[4, 0, 1]],  # sos a b
268           [[0, 1, 3]],  # a b eos
304           [[2, 2, 0],   # c c a
305            [2, 0, 3]],  # c a eos
nmt-master/nmt/utils/misc_utils.py
1 # Copyright 2017 Google Inc. All Rights Reserved.
2 #
4 # you may not use this file except in compliance with the License.
5 # You may obtain a copy of the License at
6 #
7 #     http://www.apache.org/licenses/LICENSE-2.0
8 #
9 # Unless required by applicable law or agreed to in writing, software
11 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12 # See the License for the specific language governing permissions and
13 # limitations under the License.
14 # ==============================================================================
16 """Generally useful utility functions."""
34   # LINT.IfChange
36   # LINT.ThenChange(<pwd>/nmt/copy.bara.sky)
43   """Exponentiation with catching of overflow error."""
52   """Take a start time, print elapsed duration, and return a new time."""
59   """Similar to print but with support to flush and output to a file."""
68   # stdout
80   """Print hparams, can skip keys based on pattern."""
90   """Load hparams from an existing model directory."""
107   """Override hparams values with existing standard hparams config."""
116   """Save hparams."""
124   """Print the shape and value of a tensor at test time. Return a new tensor."""
131   """Add a new summary to the current summary_writer.
  Useful to log things that are not part of the training graph, e.g., tag=BLEU.
  """
140   # GPU options:
141   # https://www.tensorflow.org/versions/r0.10/how_tos/using_gpu/index.html
147   # CPU threads options
157   """Convert a sequence words into sentence."""
165   """Convert a sequence of bpe words into sentence."""
174     else:  # end of a word
182   """Decode a text in SPM (https://github.com/google/sentencepiece) format."""
nmt-master/nmt/utils/misc_utils_test.py
1 # Copyright 2017 Google Inc. All Rights Reserved.
2 #
4 # you may not use this file except in compliance with the License.
5 # You may obtain a copy of the License at
6 #
7 #     http://www.apache.org/licenses/LICENSE-2.0
8 #
9 # Unless required by applicable law or agreed to in writing, software
11 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12 # See the License for the specific language governing permissions and
13 # limitations under the License.
14 # ==============================================================================
16 """Tests for vocab_utils."""
nmt-master/nmt/utils/nmt_utils.py
1 # Copyright 2017 Google Inc. All Rights Reserved.
2 #
4 # you may not use this file except in compliance with the License.
5 # You may obtain a copy of the License at
6 #
7 #     http://www.apache.org/licenses/LICENSE-2.0
8 #
9 # Unless required by applicable law or agreed to in writing, software
11 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12 # See the License for the specific language governing permissions and
13 # limitations under the License.
14 # ==============================================================================
16 """Utility functions specifically for NMT."""
42   """Decode a test set and compute a score according to the evaluation task."""
43   # Decode
81   # Evaluation
97   """Given batch decoding outputs, select a sentence and turn to text."""
99   # Select a sentence
102   # If there is an eos symbol in outputs, cut them at that point.
nmt-master/nmt/utils/standard_hparams_utils.py
1 # Copyright 2017 Google Inc. All Rights Reserved.
2 #
4 # you may not use this file except in compliance with the License.
5 # You may obtain a copy of the License at
6 #
7 #     http://www.apache.org/licenses/LICENSE-2.0
8 #
9 # Unless required by applicable law or agreed to in writing, software
11 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12 # See the License for the specific language governing permissions and
13 # limitations under the License.
14 # ==============================================================================
16 """standard hparams utils."""
27       # Data
37       # Networks
50       # Attention mechanisms
56       # Train
70       # Data constraints
78       # Data format
85       # Misc
88       epoch_step=0,  # record where we were within an epoch.
95       # only enable beam search during inference when beam_width > 0.
103       # For inference
110       # Language model
nmt-master/nmt/utils/vocab_utils.py
1 # Copyright 2017 Google Inc. All Rights Reserved.
2 #
4 # you may not use this file except in compliance with the License.
5 # You may obtain a copy of the License at
6 #
7 #     http://www.apache.org/licenses/LICENSE-2.0
8 #
9 # Unless required by applicable law or agreed to in writing, software
11 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12 # See the License for the specific language governing permissions and
13 # limitations under the License.
14 # ==============================================================================
16 """Utility to handle vocabularies."""
30 # word level special token
36 # char ids 0-255 come from utf-8 encoding bytes
37 # assign 256-300 to special chars
38 BOS_CHAR_ID = 256  # <begin sentence>
39 EOS_CHAR_ID = 257  # <end sentence>
40 BOW_CHAR_ID = 258  # <begin word>
41 EOW_CHAR_ID = 259  # <end word>
42 PAD_CHAR_ID = 260  # <padding>
44 DEFAULT_CHAR_MAXLEN = 50  # max number of chars for each word.
48   """Given string and length, convert to byte seq of at most max_length.

  This process mimics docqa/elmo's preprocessing:
  https://github.com/allenai/document-qa/blob/master/docqa/elmo/data.py

  Note that we make use of BOS_CHAR_ID and EOS_CHAR_ID in iterator_utils.py & 
  our usage differs from docqa/elmo.

  Args:
    text: tf.string tensor of shape []
    max_length: max number of chars for each word.

  Returns:
    A tf.int32 tensor of the byte encoded text.
  """
76   """Given a sequence of strings, map to sequence of bytes.

  Args:
    tokens: A tf.string tensor

  Returns:
    A tensor of shape words.shape + [bytes_per_word] containing byte versions
    of each word.
  """
113   """Check if vocab_file doesn't exist, create from corpus_file."""
118       # Verify if the vocab starts with unk, sos, eos
119       # If not, prepend those tokens & generate a new vocab file
144   """Creates vocab tables for src_vocab_file and tgt_vocab_file."""
156   """Load embed_file into a python dictionary.

  Note: the embed_file should be a Glove/word2vec formatted txt file. Assuming
  Here is an exampe assuming embed_size=5:

  the -0.071549 0.093459 0.023738 -0.090339 0.056123
  to 0.57346 0.5417 -0.23477 -0.3624 0.4037
  and 0.20327 0.47348 0.050877 0.002103 0.060547

  For word2vec format, the first line will be: <num_words> <emb_size>.

  Args:
    embed_file: file path to the embedding file.
  Returns:
    a dictionary that maps word to vector, and the size of embedding dimensions.
  """
181         if len(tokens) == 2:  # header line
nmt-master/nmt/utils/vocab_utils_test.py
1 # Copyright 2017 Google Inc. All Rights Reserved.
2 #
4 # you may not use this file except in compliance with the License.
5 # You may obtain a copy of the License at
6 #
7 #     http://www.apache.org/licenses/LICENSE-2.0
8 #
9 # Unless required by applicable law or agreed to in writing, software
11 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
12 # See the License for the specific language governing permissions and
13 # limitations under the License.
14 # ==============================================================================
16 """Tests for vocab_utils."""
32     # Create a vocab file
41     # Call vocab_utils
47     # Assert: we expect the code to add  <unk>, <s>, </s> and
48     # create a new vocab file
nmt-master/nmt/utils/__init__.py
nmt-master/nmt/__init__.py
